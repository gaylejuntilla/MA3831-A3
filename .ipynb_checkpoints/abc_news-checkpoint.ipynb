{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0e0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /Users/gaylejuntilla/MA3831_A3/.git/\r\n"
     ]
    }
   ],
   "source": [
    "! git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4ac4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 7717188] Initialise abc_news.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Initialise abc_news.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff7220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 12, done.\n",
      "Counting objects: 100% (12/12), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (11/11), done.\n",
      "Writing objects: 100% (11/11), 3.21 KiB | 3.21 MiB/s, done.\n",
      "Total 11 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), done.\u001b[K\n",
      "To https://github.com/gaylejuntilla/MA3831-A3.git\n",
      "   cd64767..7717188  main -> main\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bed02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run headless (no UI)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "class ABCNewsScraper:\n",
    "    def __init__(self, start_url, cutoff_date):\n",
    "        self.start_url = start_url\n",
    "        self.cutoff_date = datetime.strptime(cutoff_date, \"%B %Y\")  # Convert string to datetime\n",
    "        self.articles = pd.DataFrame(columns=['headline', 'title', 'date', 'content', 'url'])  # Store data\n",
    "        self.driver = webdriver.Chrome()  # Initialize WebDriver\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraper: loads page, extracts articles, stops at cutoff date.\"\"\"\n",
    "        self.driver.get(self.start_url)\n",
    "        time.sleep(3)  # Let page load\n",
    "        \n",
    "        # Step 1: Load articles until the cutoff date\n",
    "        self.load_articles_until_cutoff()\n",
    "        \n",
    "        # Step 2: Extract articles after loading all\n",
    "        self.extract_articles()\n",
    "        \n",
    "        self.driver.quit()  # Close browser\n",
    "        self.save_data_to_file()\n",
    "\n",
    "    def load_articles_until_cutoff(self):\n",
    "        \"\"\"Load articles until the cutoff date or until no more articles are available.\"\"\"\n",
    "        while True:\n",
    "            current_articles = self.driver.find_elements(By.CSS_SELECTOR, 'article.article-selector')  # Adjust selector!\n",
    "\n",
    "            # Check if we have enough articles and if we've reached the cutoff date\n",
    "            if current_articles:\n",
    "                latest_article_date = self.get_article_date(current_articles[-1])  # Check the date of the last loaded article\n",
    "                if latest_article_date < self.cutoff_date:\n",
    "                    print(\"Reached cutoff date. Stopping loading.\")\n",
    "                    break\n",
    "\n",
    "            # Try clicking \"Load More\" to get more articles\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.load-more-selector'))  # Adjust selector!\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                time.sleep(2)  # Wait for more articles to load\n",
    "            except:\n",
    "                print(\"No more 'Load More' button. Ending loading.\")\n",
    "                break\n",
    "    \n",
    "    def extract_articles(self):\n",
    "        \"\"\"Extracts article details: headline, date, content, and URL.\"\"\"\n",
    "        ul_element = self.driver.find_element(By.XPATH, '//*[@id=\"anchor-104815542\"]/div/div/div/ul')\n",
    "        articles = ul_element.find_elements(By.TAG_NAME, 'li') \n",
    "\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                \n",
    "                # Extract date\n",
    "                article_date_str = article.find_element(By.XPATH, \".//time[1][@datetime]\").get_attribute('datetime')\n",
    "\n",
    "                # Convert the UTC datetime string to a datetime object\n",
    "                article_date_utc = datetime.strptime(article_date_str, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "\n",
    "                # Format it to just Day, Month, and Year\n",
    "                article_date = article_date_utc.strftime(\"%d %B %Y\") # convert to local time\n",
    "                \n",
    "                # Stop if we've reached the cutoff date\n",
    "                if article_date < self.cutoff_date:\n",
    "                    break\n",
    "                    \n",
    "                # Extract the article URL\n",
    "                link_element = article.find_element(By.TAG_NAME, 'a')\n",
    "                article_url = link_element.get_attribute(\"href\")\n",
    "\n",
    "                # Extract headline\n",
    "                headline = article.find_element(By.CSS_SELECTOR, \"a[data-component='Link']\").text \n",
    "                \n",
    "                # Extract content\n",
    "                self.driver.execute_script(\"window.open('');\")  # Open new tab\n",
    "                self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "                self.driver.get(article_url)\n",
    "                time.sleep(3) # time to load article\n",
    "                \n",
    "                title = self.driver.find_elements(By.CSS_SELECTOR, \"h1[data-component='Typography']\").text\n",
    "\n",
    "                content_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div[class*=\"ArticleRender_article\"] p')\n",
    "                content = \" \".join([p.text for p in paragraphs])\n",
    "\n",
    "                # Close tab and switch back\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "                # Add data to DataFrame\n",
    "                self.articles.loc[len(self.articles)] = [headline, article_date.strftime(\"%d %B %Y\"), content, article_url]\n",
    "                print(f\"Collected: {headline}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "    def get_article_date(self, article):\n",
    "        \"\"\"Extracts the date of an article.\"\"\"\n",
    "        try:\n",
    "            date_element = article.find_element(By.CSS_SELECTOR, 'span.date-selector')  # Adjust selector!\n",
    "            article_date = datetime.strptime(date_element.text, \"%d %B %Y\")  # Convert to datetime\n",
    "            return article_date\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting date: {e}\")\n",
    "            return datetime.min  # Return a very old date if extraction fails\n",
    "    \n",
    "    def save_data_to_file(self):\n",
    "        \"\"\"Saves the scraped data to a CSV file.\"\"\"\n",
    "        self.articles.to_csv(\"abc_news_articles.csv\", index=False)\n",
    "        print(\"Data saved to abc_news_articles.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e71920e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main fd5e95e] add article title and path to content\n",
      " 1 file changed, 14 insertions(+), 13 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 644 bytes | 644.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/gaylejuntilla/MA3831-A3.git\n",
      "   650565c..fd5e95e  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"add article title and path to content\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02baa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
