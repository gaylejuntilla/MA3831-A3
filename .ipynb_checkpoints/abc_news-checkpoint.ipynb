{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0e0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /Users/gaylejuntilla/MA3831_A3/.git/\r\n"
     ]
    }
   ],
   "source": [
    "! git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4ac4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 7717188] Initialise abc_news.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Initialise abc_news.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff7220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 12, done.\n",
      "Counting objects: 100% (12/12), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (11/11), done.\n",
      "Writing objects: 100% (11/11), 3.21 KiB | 3.21 MiB/s, done.\n",
      "Total 11 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), done.\u001b[K\n",
      "To https://github.com/gaylejuntilla/MA3831-A3.git\n",
      "   cd64767..7717188  main -> main\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bed02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run headless (no UI)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "class WebCrawler:\n",
    "    def __init__(self, start_url):\n",
    "        \n",
    "        self.driver = webdriver.Chrome('chromedriver', options=options)\n",
    "        self.start_url = start_url\n",
    "        self.comments = pd.Dataframe(columns=['Headline', 'Content', 'Date'])\n",
    "\n",
    "        self.max_articles = max_articles  # Max articles to scrape\n",
    "        self.articles_scraped = 0  # Counter for scraped articles\n",
    "\n",
    "    def open_page(self):\n",
    "        # Open the initial page\n",
    "        self.driver.get(self.start_url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    def extract_article_date(self, article_element):\n",
    "        # Example: Extract article publication date (adjust the XPath or method as needed)\n",
    "        try:\n",
    "            date_str = article_element.find_element(By.XPATH, './/span[@class=\"publish-date\"]').text\n",
    "            article_date = datetime.strptime(date_str, \"%d %B %Y\")  # Example date format: \"22 September 2024\"\n",
    "            return article_date\n",
    "        except Exception as e:\n",
    "            print(f\"Could not extract date: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_articles(self):\n",
    "        # Extract all article links and dates on the page\n",
    "        articles = self.driver.find_elements(By.XPATH, '//a[contains(@class, \"article-link-class\")]')\n",
    "        article_data = []\n",
    "        for article in articles:\n",
    "            article_url = article.get_attribute('href')\n",
    "            title = article.find_element(By.XPATH, './/h3').text\n",
    "            article_date = self.extract_article_date(article)\n",
    "            \n",
    "            # Filter out articles before September 2024\n",
    "            if article_date and article_date >= datetime(2024, 9, 1):\n",
    "                article_data.append({'url': article_url, 'title': title, 'date': article_date})\n",
    "        return article_data\n",
    "\n",
    "    def load_more_articles(self):\n",
    "        # Check if the \"Load More\" button exists and is clickable\n",
    "        try:\n",
    "            load_more_button = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[@class=\"load-more-button-class\"]'))\n",
    "            )\n",
    "            load_more_button.click()\n",
    "            time.sleep(5)  # Wait for new articles to load\n",
    "        except Exception as e:\n",
    "            print(f\"Error or no more 'Load More' button: {e}\")\n",
    "\n",
    "    def crawl(self):\n",
    "        # Start the crawling process\n",
    "        self.open_page()\n",
    "\n",
    "        all_articles = []\n",
    "        unique_articles = set()  # To ensure no duplicate articles\n",
    "\n",
    "        while self.articles_scraped < self.max_articles:\n",
    "            # Extract articles from the page\n",
    "            articles = self.extract_articles()\n",
    "\n",
    "            # Filter out articles that have already been scraped (use URL or Title)\n",
    "            for article in articles:\n",
    "                if article['url'] not in unique_articles and self.articles_scraped < self.max_articles:\n",
    "                    unique_articles.add(article['url'])\n",
    "                    all_articles.append(article)\n",
    "                    self.articles_scraped += 1\n",
    "\n",
    "            # Stop if we've reached the max limit\n",
    "            if self.articles_scraped >= self.max_articles:\n",
    "                break\n",
    "\n",
    "            # Attempt to load more articles\n",
    "            self.load_more_articles()\n",
    "\n",
    "            # Optionally, you can add a condition to stop if no new articles are found\n",
    "            if len(articles) == 0:\n",
    "                print(\"No more articles to load.\")\n",
    "                break\n",
    "\n",
    "        return all_articles\n",
    "\n",
    "    def close(self):\n",
    "        # Close the WebDriver after scraping\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the WebCrawler with the starting URL\n",
    "    crawler = WebCrawler('https://www.abc.net.au/news/elections/federal-election-2025')\n",
    "\n",
    "    # Start crawling and collect articles\n",
    "    articles = crawler.crawl()\n",
    "\n",
    "    # Print the collected articles\n",
    "    for article in articles:\n",
    "        print(f\"Title: {article['title']}, URL: {article['url']}, Date: {article['date']}\")\n",
    "\n",
    "    # Close the browser\n",
    "    crawler.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
