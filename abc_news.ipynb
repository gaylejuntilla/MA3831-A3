{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0e0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /Users/gaylejuntilla/MA3831_A3/.git/\r\n"
     ]
    }
   ],
   "source": [
    "! git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4ac4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 7717188] Initialise abc_news.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Initialise abc_news.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff7220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 12, done.\n",
      "Counting objects: 100% (12/12), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (11/11), done.\n",
      "Writing objects: 100% (11/11), 3.21 KiB | 3.21 MiB/s, done.\n",
      "Total 11 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), done.\u001b[K\n",
      "To https://github.com/gaylejuntilla/MA3831-A3.git\n",
      "   cd64767..7717188  main -> main\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4e8ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from webdriver_manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from webdriver_manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver_manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver_manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver_manager) (2024.12.14)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver_manager\n",
      "Successfully installed webdriver_manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bed02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run headless (no UI)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "class ABCNewsScraper:\n",
    "    def __init__(self, start_url, cutoff_date=\"1 October 2024\"):\n",
    "        self.start_url = start_url\n",
    "        self.cutoff_date = datetime.strptime(cutoff_date, \"%d %B %Y\")  # Convert string to datetime\n",
    "        self.articles = pd.DataFrame(columns=['headline', 'title', 'date', 'content', 'url'])  # Store data\n",
    "        self.driver = webdriver.Chrome('chromedriver',chrome_options=options)  # Initialize WebDriver\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraper: loads page, extracts articles, stops at cutoff date.\"\"\"\n",
    "        self.driver.get(self.start_url)\n",
    "        time.sleep(3)  # Let page load\n",
    "        \n",
    "        # Step 1: Load articles until the cutoff date\n",
    "        self.load_articles_until_cutoff()\n",
    "        \n",
    "        # Step 2: Extract articles after loading all\n",
    "        self.extract_articles()\n",
    "        \n",
    "        self.driver.quit()  # Close browser\n",
    "        self.save_data_to_file()\n",
    "\n",
    "    def load_articles_until_cutoff(self):\n",
    "    \"\"\"Keep loading more articles until the last article is older than the cutoff date.\"\"\"\n",
    "        while True:\n",
    "            # Get all currently loaded articles\n",
    "            articles = self.driver.find_elements(By.CSS_SELECTOR, 'ul[aria-labelledby=\"Latest election articles\"] li')\n",
    "\n",
    "            if not articles:\n",
    "                print(\"No articles found on page. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # Find the date of the last article\n",
    "            try:\n",
    "                last_article = articles[-1]\n",
    "                last_date_str = last_article.find_element(By.XPATH, \".//time[1]\").get_attribute(\"datetime\")\n",
    "                last_article_date_utc = datetime.strptime(last_date_str, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "                last_article_date = self.convert_to_brisbane_time(last_article_date_utc)\n",
    "\n",
    "                # Stop loading if the last article's date is older than the cutoff\n",
    "                if last_article_date < self.cutoff_date:\n",
    "                    print(\"Reached cutoff date. Stopping 'Load More' clicks.\")\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting date from last article: {e}\")\n",
    "                break  # Stop in case of unexpected errors\n",
    "\n",
    "            # Try clicking \"Load More\" to get more articles\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-component=\"PaginationLoadMoreButton\"]')))\n",
    "                load_more_button.click()\n",
    "                time.sleep(2)  # Wait for more articles to load\n",
    "\n",
    "            except:\n",
    "                print(\"No more 'Load More' button. Ending loading.\")\n",
    "                break\n",
    "    \n",
    "    def convert_to_brisbane_time(self, utc_datetime):\n",
    "    \"\"\"Converts a UTC datetime string to Brisbane time and formats it as 'DD Month YYYY'.\"\"\"\n",
    "    \n",
    "        # Define UTC timezone\n",
    "        utc_zone = timezone.utc\n",
    "\n",
    "        # Define Brisbane timezone (UTC+10, no daylight savings)\n",
    "        brisbane_zone = timezone(timedelta(hours=10))\n",
    "\n",
    "        # Convert to Brisbane time\n",
    "        brisbane_datetime = utc_datetime.replace(tzinfo=utc_zone).astimezone(brisbane_zone)\n",
    "        \n",
    "        # Format as 'DD Month YYYY'\n",
    "        return brisbane_datetime\n",
    "    \n",
    "    def extract_articles(self):\n",
    "    \"\"\"Extracts article details: headline, date, content, and URL.\"\"\"\n",
    "        \n",
    "        articles = self.driver.find_elements(By.CSS_SELECTOR, 'ul[aria-labelledby=\"Latest election articles\"] li')\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                \n",
    "                # Extract date\n",
    "                article_date_str = article.find_element(By.XPATH, \".//time[1][@datetime]\").get_attribute('datetime')\n",
    "\n",
    "                # Convert the UTC datetime string to a datetime object\n",
    "                article_date_utc = datetime.strptime(article_date_str, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "\n",
    "                # Format it to just Day, Month, and Year\n",
    "                article_date = self.convert_to_brisbane_time(article_date_utc)\n",
    "                \n",
    "                # Stop if we've reached the cutoff date\n",
    "                if article_date < self.cutoff_date:\n",
    "                    break\n",
    "                    \n",
    "                # Extract the article URL\n",
    "                link_element = article.find_element(By.TAG_NAME, 'a')\n",
    "                article_url = link_element.get_attribute(\"href\")\n",
    "\n",
    "                # Extract headline\n",
    "                headline = article.find_element(By.CSS_SELECTOR, \"a[data-component='Link']\").text \n",
    "                \n",
    "                # Extract content\n",
    "                self.driver.execute_script(\"window.open('');\")  # Open new tab\n",
    "                self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "                self.driver.get(article_url)\n",
    "                time.sleep(3) # time to load article\n",
    "                \n",
    "                title = self.driver.find_element(By.CSS_SELECTOR, \"h1[data-component='Typography']\").text\n",
    "\n",
    "                content_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div[class*=\"ArticleRender_article\"] p')\n",
    "                content = \" \".join([p.text for p in content_elements])\n",
    "\n",
    "                # Close tab and switch back\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "                # Add data to DataFrame\n",
    "                self.articles.loc[len(self.articles)] = [headline, title, article_date, content, article_url]\n",
    "                print(f\"Collected: {headline}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "    \n",
    "    def save_data_to_file(self):\n",
    "        \"\"\"Saves the scraped data to a CSV file.\"\"\"\n",
    "        self.articles.to_csv(\"abc_news_articles.csv\", index=False)\n",
    "        print(\"Data saved to abc_news_articles.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e71920e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 5ddb222] fix small errors\r\n",
      " 1 file changed, 10 insertions(+), 22 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"fix small errors\"\n",
    "#!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02baa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
