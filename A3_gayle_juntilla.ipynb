{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8010bf54",
   "metadata": {},
   "source": [
    "# Compilation of Notebooks into one\n",
    "## Original Notebooks and runs in github link: https://github.com/gaylejuntilla/MA3831-A3\n",
    "NOTE: This notebook has not been run and will probably not run since some codes require specific virtual environments and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general list of imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "import torch\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7968315",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ac996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "class ABCNewsCrawler:\n",
    "    def __init__(self, start_url, cutoff_date=\"1 January 2025\"):\n",
    "        \"\"\"Initialise the instance\"\"\"\n",
    "        self.start_url = start_url\n",
    "        self.cutoff_date = datetime.strptime(cutoff_date, \"%d %B %Y\")  # Convert string to datetime\n",
    "        self.articles = pd.DataFrame(columns=['headline', 'title', 'date', 'content', 'url'])  # Store data\n",
    "        \n",
    "        # Initialize the service object for ChromeDriverManager\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        \n",
    "        # Initialize WebDriver with service and options\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')  # Run headless (no UI)\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        self.driver = webdriver.Chrome(service=service, options=options) \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Runs the scraper: loads articles until cutoff date, extracts articles.\"\"\"\n",
    "        self.driver.get(self.start_url)\n",
    "        time.sleep(3)  # Let page load\n",
    "        \n",
    "        # Step 1: Load articles until the cutoff date\n",
    "        self.load_articles_until_cutoff()\n",
    "        \n",
    "        # Step 2: Extract articles after loading all\n",
    "        self.extract_articles()\n",
    "        \n",
    "        self.driver.quit()  # Close browser\n",
    "        #print(self.articles)\n",
    "\n",
    "    def load_articles_until_cutoff(self):\n",
    "        \"\"\"Keep loading more articles until the last article is older than the cutoff date.\"\"\"\n",
    "    \n",
    "        while True:\n",
    "            # Get all currently loaded articles\n",
    "            articles = self.driver.find_elements(By.CSS_SELECTOR, 'ul[aria-labelledby=\"Latest election articles\"] li')\n",
    "\n",
    "            if not articles:\n",
    "                print(\"No articles found on page. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # Find the date of the last article\n",
    "            try:\n",
    "                # last article in list\n",
    "                last_article = articles[-1] \n",
    "                # find date attribute\n",
    "                last_date_str = last_article.find_element(By.XPATH, \".//time[1]\").get_attribute(\"datetime\")\n",
    "                # convert to a datetime object\n",
    "                last_article_date_utc = datetime.strptime(last_date_str, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "                # convert to brisbane time\n",
    "                last_article_date = self.convert_to_brisbane_time(last_article_date_utc)\n",
    "\n",
    "                # Stop loading if the last article's date is older than the cutoff\n",
    "                if last_article_date < self.cutoff_date:\n",
    "                    print(\"Reached cutoff date. Stopping 'Load More' clicks.\")\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting date from last article: {e}\")\n",
    "                break  # Stop in case of unexpected errors\n",
    "\n",
    "            # Try clicking \"Load More\" to get more articles\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-component=\"PaginationLoadMoreButton\"]')))\n",
    "                load_more_button.click()\n",
    "                time.sleep(2)  # Wait for more articles to load\n",
    "\n",
    "            except:\n",
    "                print(\"No more 'Load More' button. Ending loading.\")\n",
    "                break\n",
    "    \n",
    "    def convert_to_brisbane_time(self, utc_datetime):\n",
    "        \"\"\"Converts a UTC datetime string to Brisbane time and formats it as 'DD Month YYYY'.\"\"\"\n",
    "    \n",
    "        # Define UTC timezone\n",
    "        utc_zone = timezone.utc\n",
    "\n",
    "        # Define Brisbane timezone (UTC+10, no daylight savings)\n",
    "        brisbane_zone = timezone(timedelta(hours=10))\n",
    "\n",
    "        # Convert to Brisbane time\n",
    "        brisbane_datetime = utc_datetime.replace(tzinfo=timezone.utc).astimezone(brisbane_zone)\n",
    "        \n",
    "        # Makes the date timezone independent, consistent with the other domains for processing\n",
    "        brisbane_datetime_naive = brisbane_datetime.replace(tzinfo=None)\n",
    "        \n",
    "        # Format as 'DD Month YYYY'\n",
    "        return brisbane_datetime_naive\n",
    "    \n",
    "    def extract_articles(self):\n",
    "        \"\"\"Extracts article details: headline, date, content, and URL.\"\"\"\n",
    "        \n",
    "        # Extracts all article elements into a list\n",
    "        articles = self.driver.find_elements(By.CSS_SELECTOR, 'ul[aria-labelledby=\"Latest election articles\"] li')\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                \n",
    "                # Extract date\n",
    "                article_date_str = article.find_element(By.XPATH, \".//time[1][@datetime]\").get_attribute('datetime')\n",
    "\n",
    "                # Convert the UTC datetime string to a datetime object\n",
    "                article_date_utc = datetime.strptime(article_date_str, \"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "\n",
    "                # Convert to UTC+10 and format it to just Day, Month, and Year\n",
    "                article_date = self.convert_to_brisbane_time(article_date_utc)\n",
    "                \n",
    "                # Stop if we've reached the cutoff date\n",
    "                if article_date < self.cutoff_date:\n",
    "                    break\n",
    "                    \n",
    "                # Extract the article URL\n",
    "                link_element = article.find_element(By.TAG_NAME, 'a')\n",
    "                article_url = link_element.get_attribute(\"href\")\n",
    "\n",
    "                # Extract headline\n",
    "                headline = article.find_element(By.CSS_SELECTOR, \"a[data-component='Link']\").text \n",
    "                \n",
    "                # Extract content\n",
    "                self.driver.execute_script(\"window.open('');\")  # Open new tab\n",
    "                self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "                self.driver.get(article_url)\n",
    "                time.sleep(3) # time to load article\n",
    "                \n",
    "                # title is different from the headline so extract title as well\n",
    "                title = self.driver.find_element(By.CSS_SELECTOR, \"h1[data-component='Typography']\").text\n",
    "\n",
    "                content_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div[class*=\"ArticleRender_article\"] p')\n",
    "                content = \" \".join([p.text for p in content_elements])\n",
    "\n",
    "                # Close tab and switch back\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "                # Add data to DataFrame\n",
    "                self.articles.loc[len(self.articles)] = [headline, title, article_date, content, article_url]\n",
    "                print(f\"Collected: {headline}\")\n",
    "                \n",
    "                # wait 10 seconds between each article\n",
    "                time.sleep(10)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "\n",
    "    \n",
    "    def save_data_to_file(self):\n",
    "        \"\"\"Saves the scraped data to a CSV file.\"\"\"\n",
    "        self.articles.to_csv(\"abc_news_articles.csv\", index=False)\n",
    "        print(\"Data saved to abc_news_articles.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycrawler = ABCNewsCrawler(\"https://www.abc.net.au/news/elections/federal-election-2025\")\n",
    "mycrawler.run()\n",
    "# make sure to save\n",
    "mycrawler.save_data_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class GuardianScraper:\n",
    "    def __init__(self, start_url):\n",
    "        \"\"\"Initialise the scraper instance\"\"\"\n",
    "        self.start_url = start_url\n",
    "        self.articles = pd.DataFrame(columns=['headline', 'title', 'content', 'date', 'url'])  # DataFrame to store article data\n",
    "        # counter to keep track of pagination\n",
    "        self.current_page = 1\n",
    "\n",
    "    def scrape_articles(self, url):\n",
    "        \"\"\"Scrapes articles from the page and handles pagination.\"\"\"\n",
    "        \n",
    "        while url:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'lxml') # parses the webpage\n",
    "\n",
    "            # Step 1: Scrape articles on the current page\n",
    "            container = soup.find('main')\n",
    "            articles = container.find_all('li')\n",
    "\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    # Get article URL\n",
    "                    article_url = self.fetch_article_url(article)\n",
    "                    print(article_url)\n",
    "\n",
    "                    # Scrape the article details: title, content, date\n",
    "                    title, content, date = self.scrape_article_details(article_url)\n",
    "\n",
    "                    # Add data to DataFrame\n",
    "                    if title and content and date:\n",
    "                        new_row = pd.DataFrame({\n",
    "                                'headline': [title],  # Wrap values in lists\n",
    "                                'title': [title],\n",
    "                                'content': [content],\n",
    "                                'date': [date],\n",
    "                                'url': [article_url]\n",
    "                            })\n",
    "                        self.articles = pd.concat([self.articles, new_row], ignore_index=True)\n",
    "                        print(f\"Collected: {title}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping article: {title}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "\n",
    "                time.sleep(5)\n",
    "                \n",
    "            url = self.get_next_page(soup)\n",
    "            if not url:\n",
    "                print(\"No more pages found, stopping...\")\n",
    "                break\n",
    "            self.current_page += 1\n",
    "\n",
    "    \n",
    "    def fetch_article_url(self, article):\n",
    "        \"\"\"method to extract full URL for pagination.\"\"\"\n",
    "        base_url = \"https://www.theguardian.com\"\n",
    "        article_link = article.find('a')['href']\n",
    "        if article_link.startswith('/'):\n",
    "            article_url = base_url + article_link  # Make it a full URL\n",
    "        else:\n",
    "            article_url = article_link\n",
    "        \n",
    "        return article_url\n",
    "\n",
    "    def scrape_article_details(self, article_url):\n",
    "        \"\"\"Scrapes article content, title, and date from individual article pages.\"\"\"\n",
    "        response = requests.get(article_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            # Extracting title\n",
    "            title = soup.find('h1').text\n",
    "\n",
    "            # Extract date text\n",
    "            date_text = soup.find('span', {'class': 'dcr-u0h1qy'}).text.strip()\n",
    "\n",
    "            # The date has the time and timezone, so ot needs to be removed\n",
    "            date_text_without_time_zone = re.sub(r'\\s[A-Za-z]+$', '', date_text) \n",
    "\n",
    "            # Convert to datetime object\n",
    "            date_only = ' '.join(date_text_without_time_zone.split()[:4])\n",
    "            date_obj = datetime.strptime(date_only, \"%a %d %b %Y\")\n",
    "            \n",
    "            # keep formatting consistent\n",
    "            date = date_obj.strftime(\"%d %B %Y\")\n",
    "\n",
    "            print(date)  # For example: '30 March 2025'\n",
    "\n",
    "\n",
    "            # Extracting content\n",
    "            content_element = soup.find('article')\n",
    "            paragraphs = content_element.find_all('p')\n",
    "            content = \" \".join([p.text for p in paragraphs]).strip()\n",
    "\n",
    "            return title, content, date\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "            return None, None, None  # Return None in case of an error\n",
    "\n",
    "    def get_next_page(self, soup):\n",
    "        \"\"\"Finds the next page URL from pagination links at the bottom of the page.\"\"\"\n",
    "        # Find pagination section\n",
    "        pagination_links = soup.find('div', {'class': 'dcr-stdtpu'})  \n",
    "\n",
    "        if pagination_links:\n",
    "            # Find all numbered page links using regex\n",
    "            all_pages = pagination_links.find_all('a', string=re.compile(r'\\d+')) \n",
    "\n",
    "            for page_link in all_pages:\n",
    "                # if page is greater than the current counter than return URL otherwise None\n",
    "                if int(page_link.text.strip()) == self.current_page + 1:\n",
    "                    next_page_url = page_link['href'] # Get the next sequential page\n",
    "                    # handles the incomplete URL\n",
    "                    if next_page_url.startswith('/'):\n",
    "                        next_page_url = \"https://www.theguardian.com\" + next_page_url\n",
    "                    return next_page_url\n",
    "\n",
    "    def save_data_to_file(self, filename='guardian_articles.csv'):\n",
    "        \"\"\"Saves the collected articles to a CSV file.\"\"\"\n",
    "        self.articles.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://www.theguardian.com/australia-news/australian-election-2025/all\"\n",
    "left_scraper = GuardianScraper(start_url=start_url)\n",
    "left_scraper.scrape_articles(url=start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7caeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class SkyNewsScraper:\n",
    "    def __init__(self, start_url, cutoff_date=\"1 January 2025\"):\n",
    "        self.start_url = start_url\n",
    "        self.cutoff_date = datetime.strptime(cutoff_date, \"%d %B %Y\")  # Convert string to datetime\n",
    "        self.articles = pd.DataFrame(columns=['headline', 'title', 'content', 'date', 'url'])  # DataFrame to store article data\n",
    "        self.current_page = 1\n",
    "        \n",
    "    def scrape_articles(self, url):\n",
    "        \"\"\"Scrapes articles from the page and handles pagination.\"\"\"\n",
    "        \n",
    "        while url:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            articles = soup.find_all('article', {\"class\":\"storyblock\"})\n",
    "\n",
    "            if not articles:\n",
    "                print(\"No articles found on page, stopping...\")\n",
    "                break\n",
    "            \n",
    "            for article in articles:\n",
    "                try:\n",
    "                    # Extract article info\n",
    "                    article_info = self.fetch_article_info(article)\n",
    "                    article_date = datetime.strptime(article_info['date'], \"%d %B %Y\")  # Convert to datetime object\n",
    "                    \n",
    "                    # Check if this article is older than cutoff_date\n",
    "                    if article_date < self.cutoff_date:\n",
    "                        print(f\"Reached article older than cutoff date: {article_info['date']}, stopping...\")\n",
    "                        return  # Exit function to stop scraping\n",
    "\n",
    "                    # Scrape article details\n",
    "                    title, content = self.scrape_article_details(article_info['article_url'])\n",
    "\n",
    "                    if title and content:\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'headline': [article_info['headline']],\n",
    "                            'title': [title],\n",
    "                            'content': [content],\n",
    "                            'date': [article_info['date']],\n",
    "                            'url': [article_info['article_url']]\n",
    "                        })\n",
    "                        self.articles = pd.concat([self.articles, new_row], ignore_index=True)\n",
    "                        print(f\"Collected: {title}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping article: {title}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "\n",
    "                #time.sleep(5)\n",
    "                \n",
    "            # Step 2: Find the next page (pagination)\n",
    "            url = self.get_next_page(soup)\n",
    "            if not url:\n",
    "                print(\"No more pages found, stopping...\")\n",
    "                break\n",
    "            self.current_page += 1\n",
    "    \n",
    "    def fetch_article_info(self, article):\n",
    "        \"\"\"Helper method to extract article information like headline and URL.\"\"\"\n",
    "        attributes = article.find('h4', {'class': \"storyblock_title\"})\n",
    "        article_url = attributes.find('a')['href']\n",
    "        headline = attributes.find('a').text\n",
    "        date_str = article.find('time')['datetime']  # Extract the date string\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S%z\")  # Convert to datetime object\n",
    "        date = date_obj.strftime(\"%d %B %Y\")\n",
    "\n",
    "        return {'headline': headline, 'article_url': article_url, 'date': date} \n",
    "\n",
    "    def scrape_article_details(self, article_url):\n",
    "        \"\"\"Scrapes article content, title, date, and other details from individual article pages.\"\"\"\n",
    "        response = requests.get(article_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            # Extracting title\n",
    "            title = soup.find('h1').text\n",
    "\n",
    "            # Extracting content (Assuming paragraphs are in 'div' with class 'content__article-body')\n",
    "            content_element = soup.find('div', {'id': 'story-primary'})\n",
    "            paragraphs = content_element.find_all('p')\n",
    "            content = \" \".join([p.text for p in paragraphs]).strip()\n",
    "\n",
    "            return title, content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "            return None, None, None  # Return None in case of an error\n",
    "\n",
    "    def get_next_page(self, soup):\n",
    "        \"\"\"Finds the next page URL from pagination links at the bottom of the page.\"\"\"\n",
    "        pagination_links = soup.find('ul', {'class': 'page-numbers'})  # Find pagination section\n",
    "\n",
    "        if pagination_links:\n",
    "            all_pages = pagination_links.find_all('a', string=re.compile(r'\\d+'))  # Find all numbered page links\n",
    "\n",
    "            # Otherwise, continue to the next page in sequence\n",
    "            for page_link in all_pages:\n",
    "                if int(page_link.text.strip()) == self.current_page + 1:\n",
    "                    return page_link['href']  # Get the next sequential page\n",
    "\n",
    "    def save_data_to_file(self, filename='skynews_articles.csv'):\n",
    "        \"\"\"Saves the collected articles to a CSV file.\"\"\"\n",
    "        self.articles.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67425bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skynews_url = \"https://www.skynews.com.au/australia-news/politics\"\n",
    "right_scraper = SkyNewsScraper(start_url=skynews_url)\n",
    "right_scraper.scrape_articles(url=skynews_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5f118",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian = pd.read_csv('guardian_articles.csv')\n",
    "abc = pd.read_csv('abc_news_articles.csv')\n",
    "skynews = pd.read_csv('skynews_articles_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_data(df, source):\n",
    "    \"\"\"Remove escape sequences and spacing\"\"\"\n",
    "    df = df.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    string_columns = ['headline', 'title', 'content']\n",
    "    for column in string_columns:\n",
    "         # Remove common escape sequences\n",
    "        df[column] = df[column].apply(lambda x: re.sub(r'\\\\[nrt\\'\"\\\\]', ' ', str(x)))\n",
    "        df[column] = df[column].apply(lambda x: re.sub(r'\\s+', ' ', str(x)).strip())\n",
    "        # Specifically replace escaped apostrophes (\\' becomes ')\n",
    "        df[column] = df[column].apply(lambda x: re.sub(r'\\\\\"', '\"', str(x)))\n",
    "\n",
    "    df['date'] = df['date'].apply(parse_date)\n",
    "    df['date'] = df['date'].dt.date\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['source'] = source\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8bf3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    \"\"\"Convert the dates to datetime objects\"\"\"\n",
    "    try:\n",
    "        # Try parsing as date-time format (like '2025-04-01 18:18:43')\n",
    "        return pd.to_datetime(date_str, errors='raise')  # Raise error if parsing fails\n",
    "    except Exception:\n",
    "        try:\n",
    "            # If the first attempt fails, try date-only format (like '10 March 2025')\n",
    "            return pd.to_datetime(date_str, format='%d %B %Y', errors='coerce')\n",
    "        except Exception:\n",
    "            # If both attempts fail, return NaT (Not a Time)\n",
    "            return pd.NaT\n",
    "guardian_df = clean_raw_data(guardian, 'theguardian')\n",
    "abc_df = clean_raw_data(abc, 'abc')\n",
    "skynews_df = clean_raw_data(skynews, 'skynews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting every 5th row to reduce size\n",
    "skynews_df = skynews_df.iloc[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c9be2",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325df061",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "corpus = pd.read_csv('corpus.csv')\n",
    "corpus_df = corpus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401528c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_leader_pairs = {\n",
    "    \"Australian Labor Party\": \"Anthony Albanese\",\n",
    "    \"Liberal Party\": \"Peter Dutton\",\n",
    "    \"National Party\": \"David Littleproud\",\n",
    "    \"Coalition\": \"Peter Dutton\",\n",
    "    \"Australian Greens\": \"Adam Bandt\",\n",
    "    \"One Nation\": \"Pauline Hanson\",\n",
    "    \"Australia's Voice\": \"Fatima Payman\",\n",
    "    \"Centre Alliance\": \"Rebekha Sharkie\",\n",
    "    \"David Pocock\": \"David Pocock\",\n",
    "    \"Jacqui Lambie Network\": \"Jacqui Lambie\",\n",
    "    \"Katter's Australian Party\": \"Robbie Katter\",\n",
    "    \"People First Party\": \"Gerard Rennick\",\n",
    "    \"United Australia Party\": \"Ralph Babet\",\n",
    "    \"Animal Justice Party\": \"Angela Pollard\",\n",
    "    \"Australian Christians\": \"Maryka Groenewald\",\n",
    "    \"Australian Citizens Party\": \"Craig Isherwood\",\n",
    "    \"Australian Democrats\": \"Lyn Allison\",\n",
    "    \"Better Together Party\": \"Lucy Bradlow\",\n",
    "    \"Fusion Party\": \"Drew Wolfendale\",\n",
    "    \"Family First Party\": \"Lyle Shelton\",\n",
    "    \"The Great Australian Party\": \"Rod Culleton\",\n",
    "    \"Indigenous-Aboriginal Party\": \"'Uncle' Owen Whyman\",\n",
    "    \"Kim for Canberra\": \"Kim Rubenstein\",\n",
    "    \"Legalise Cannabis\": \"Michael Balderstone\",\n",
    "    \"Libertarian Party\": \"Anthony Bull\",\n",
    "    \"Shooters, Fishers, and Farmers Party\": \"Robert Brown\",\n",
    "    \"Socialist Alliance\": \"Jacob Andrewartha\",\n",
    "    \"Sustainable Australia Party\": \"Celeste Ackerly\",\n",
    "    \"Trumpet of Patriots\": \"Suellen Wrightson\",\n",
    "    \"Victorian Socialists\": \"Collective leadership\",\n",
    "    \"Katter's Australian Party\": \"Bob Katter\",\n",
    "    \"Independents\": \"Independents\"\n",
    "}\n",
    "abbreviations_parties = {\n",
    "    \"Greens\": \"Australian Greens\",\n",
    "    \"Labor\": \"Australian Labor Party\",\n",
    "    \"Liberal\": \"Coalition\",\n",
    "    \"National\": \"Coalition\"\n",
    "}\n",
    "abbreviations_leaders = {\n",
    "    \"Dutton\": \"Peter Dutton\",\n",
    "    \"Albanese\": \"Anthony Albanese\",\n",
    "    \"Bandt\": \"Adam Bandt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_party_and_leader_entities(nlp):\n",
    "    \"\"\"Creates a rule-based system for important entities\"\"\"\n",
    "    # Creating a new named entity ruler before the NER pipe\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    \n",
    "    # Prepare a list of patterns for full names and abbreviations\n",
    "    patterns = []\n",
    "    \n",
    "    # Add patterns for full party names and their leaders\n",
    "    for party, leader in party_leader_pairs.items():\n",
    "        patterns.append({\"label\": \"ORG\", \"pattern\": party})  # Party as ORG\n",
    "        patterns.append({\"label\": \"PERSON\", \"pattern\": leader})  # Leader as PERSON\n",
    "    \n",
    "    # Add patterns for abbreviations\n",
    "    for short_party_name, full_party_name in abbreviations_parties.items():\n",
    "        patterns.append({\"label\": \"ORG\", \"pattern\": short_party_name})\n",
    "    \n",
    "    for short_name, full_name in abbreviations_leaders.items():\n",
    "        patterns.append({\"label\": \"PERSON\", \"pattern\": short_name}) \n",
    "    \n",
    "    # Add the patterns to the entity ruler\n",
    "    ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b17620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_with_spacy(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    add_party_and_leader_entities(nlp)\n",
    "    people = []\n",
    "    parties = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            if ent.text in party_leader_pairs.keys() or ent.text in abbreviations_parties.keys():\n",
    "                parties.append(ent.text)\n",
    "    return people, parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7730a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df[['headline_people', 'headline_parties']] = corpus_df['headline'].apply(lambda x: pd.Series(ner_with_spacy(x)))\n",
    "corpus_df[['content_people', 'content_parties']] = corpus_df['content'].apply(lambda x: pd.Series(ner_with_spacy(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97241561",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['headline_people'] = corpus_df['headline_people'].apply(lambda x: list(set(x)))\n",
    "corpus_df['headline_parties'] = corpus_df['headline_parties'].apply(lambda x: list(set(x)))\n",
    "corpus_df['content_people'] = corpus_df['content_people'].apply(lambda x: list(set(x)))\n",
    "corpus_df['content_parties'] = corpus_df['content_parties'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab78bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.to_csv('corpus_NER.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cc183",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ba84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to apply VADER sentiment analysis to text\n",
    "def get_vader_sentiment(text):\n",
    "    sentiment = analyzer.polarity_scores(text) # Get sentiment scores\n",
    "    return sentiment['compound']  # Compound score is the overall sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d52b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['headline_score'] = corpus_df['headline'].apply(get_vader_sentiment)\n",
    "corpus_df['headline_sentiment'] = corpus_df['headline_score'].apply(lambda x: 'positive' if x>0.05 else ('negative' if x< -0.05 else 'neutral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d73080",
   "metadata": {},
   "source": [
    "# ELMo Embeddings\n",
    "## Article content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770aa754",
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3849c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_content(content, token_limit=800):\n",
    "    # Initialize variables to count tokens and store truncated sentences\n",
    "    total_tokens = 0\n",
    "    truncated_sentences = []\n",
    "    sentences = sent_tokenize(content)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(sentence.split())  # Token count of the sentence\n",
    "        \n",
    "        # Check if adding this sentence will exceed the token limit\n",
    "        if total_tokens + sentence_tokens > token_limit:\n",
    "            break  # Stop if we've reached the limit\n",
    "        truncated_sentences.append(sentence)\n",
    "        total_tokens += sentence_tokens\n",
    "    \n",
    "    # Join the truncated sentences back into a string\n",
    "    return ' '.join(truncated_sentences)\n",
    "corpus_df['content'] = corpus_df['content'].apply(truncate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embeddings_batch(text_batch):\n",
    "    \"\"\"Function to get ELMo embeddings for a batch of text\"\"\"\n",
    "    elmo_input = tf.convert_to_tensor(text_batch, dtype=tf.string)\n",
    "    \n",
    "    # Get the embeddings from ELMo\n",
    "    elmo_output = elmo.signatures['default'](text=elmo_input)\n",
    "    embeddings = elmo_output['elmo']\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def process_in_batches(df, batch_size=16, max_length=800):\n",
    "    \"\"\"\n",
    "    Process the content column in batches, getting ELMo embeddings for each batch.\n",
    "    Pads the embeddings to ensure consistent shapes for concatenation.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        # Create batch slice\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch_text = df['content'][start:end].tolist()\n",
    "        \n",
    "        # Get ELMo embeddings for the batch\n",
    "        try:\n",
    "            batch_embeddings = get_elmo_embeddings_batch(batch_text)\n",
    "            \n",
    "            # Pad to max_length if sentence is shorter, truncate if longer\n",
    "            batch_embeddings_padded = pad_sequences(batch_embeddings.numpy(), \n",
    "                                                     maxlen=max_length, \n",
    "                                                     dtype='float32', \n",
    "                                                     padding='post', \n",
    "                                                     truncating='post')\n",
    "            \n",
    "            embeddings.append(batch_embeddings_padded)  # Add the padded embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {start}-{end}: {e}\")\n",
    "            embeddings.append(None)  # Append None in case of failure\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if (start // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {start} to {end} rows...\")\n",
    "    \n",
    "    # Concatenate all batches together\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Process just the 'content' column and return embeddings as a separate variable\n",
    "elmo_embeddings = process_in_batches(corpus_df, batch_size=16, max_length=800)\n",
    "\n",
    "# Now, elmo_embeddings holds the embeddings for the 'content' column only\n",
    "print(elmo_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da382f",
   "metadata": {},
   "source": [
    "## Headline embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headline embeddings for bias classifier\n",
    "def elmo_tokenize(texts):\n",
    "    \"\"\"Use ELMo's internal tokenization (same as used in the model).\"\"\"\n",
    "    embeddings = elmo.signatures['default'](tf.convert_to_tensor(texts))\n",
    "    return embeddings['elmo']\n",
    "\n",
    "def get_max_tokenized_length(df):\n",
    "    \"\"\"Calculate the max tokenized length using ELMo's internal tokenization.\"\"\"\n",
    "    max_length = 0\n",
    "    for text in df['headline']:\n",
    "        # Tokenize text using ELMo's tokenizer (embedding)\n",
    "        tokens = elmo_tokenize([text])  # Tokenize sentence\n",
    "        token_length = tokens.shape[1]  # Get token count (second dimension)\n",
    "        max_length = max(max_length, token_length)\n",
    "    return max_length\n",
    "\n",
    "def process_headline_embeddings(df, batch_size=32):\n",
    "    \"\"\"Process the headline column in batches, getting ELMo embeddings.\"\"\"\n",
    "    embeddings = []\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Calculate the max tokenized length once\n",
    "    max_tokenized_len = get_max_tokenized_length(df)\n",
    "    print(f\"Max tokenized sentence length: {max_tokenized_len}\")\n",
    "    \n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        # Create batch slice\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch_text = df['headline'][start:end].tolist()\n",
    "        \n",
    "        # Get ELMo embeddings for the batch\n",
    "        try:\n",
    "            batch_embeddings = elmo_tokenize(batch_text)\n",
    "            \n",
    "            batch_embeddings_padded = np.array([np.pad(\n",
    "                emb.numpy(), \n",
    "                ((0, max_tokenized_len - emb.shape[0]), (0, 0)),  # Pad to max length\n",
    "                'constant') for emb in batch_embeddings])\n",
    "            \n",
    "            embeddings.append(batch_embeddings_padded)  # Add the padded embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {start}-{end}: {e}\")\n",
    "            embeddings.append(None)  # Append None in case of failure\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if (start // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {start} to {end} rows...\")\n",
    "    \n",
    "    # Concatenate all batches together\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Call this function to process the embeddings\n",
    "elmo_headline_embeddings = process_headline_embeddings(corpus_df, batch_size=32)\n",
    "\n",
    "# Check the shape (should be num_samples, max_tokenized_len, 1024)\n",
    "print(elmo_headline_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827ea4d",
   "metadata": {},
   "source": [
    "# Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sentiment = pd.get_dummies(corpus_df['headline_sentiment']).to_numpy()\n",
    "corpus2 = corpus_df.drop(manual_indices)\n",
    "y_sentiment = pd.get_dummies(corpus2['headline_sentiment']).to_numpy()\n",
    "y2 = np.argmax(y_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ace5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_classifier(X, y, max_length, embedding_dim, num_classes=3, \n",
    "                          lstm_units=128, dropout_rate=0.3, dense_units=4,\n",
    "                          batch_size=16, epochs=5, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Train an LSTM model on the given data.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Input embeddings\n",
    "        y (ndarray): One-hot encoded labels\n",
    "        max_length (int): Max sequence length\n",
    "        embedding_dim (int): Embedding dimensionality (e.g. 1024 for ELMo, 768 for BERT)\n",
    "        num_classes (int): Number of output classes\n",
    "        lstm_units (int): Number of LSTM units\n",
    "        dropout_rate (float): Dropout rate\n",
    "        dense_units (int): Units in dense layer\n",
    "        batch_size (int): Batch size\n",
    "        epochs (int): Number of training epochs\n",
    "        validation_split (float): Fraction of data used for validation\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(X))\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        X, y, indices, test_size=validation_split, random_state=42\n",
    "    )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(max_length, embedding_dim)),\n",
    "        tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test))\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    # Converts predictions into single classes from 0-2\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    evaluation = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "    return model, history, evaluation, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elmo_sent, history_elmo_sent, eval_elmo_sent, y_pred_elmo_sent, y_true_elmo_sent = train_lstm_classifier(\n",
    "    X=elmo_embeddings,\n",
    "    y=y_sentiment,\n",
    "    max_length=800,\n",
    "    embedding_dim=1024\n",
    ")\n",
    "print(classification_report(y_true_elmo_sent, y_pred_elmo_sent, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b12ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history_elmo_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d083562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_to_label(score):\n",
    "    \"\"\"Mapping the headline VADER scores to match the supervised model scores\"\"\"\n",
    "    if score <= -0.05:\n",
    "        return 0  # negative\n",
    "    elif score >= 0.05:\n",
    "        return 2  # positive\n",
    "    else:\n",
    "        return 1  # neutral\n",
    "\n",
    "corpus2['vader_to_lstm'] = corpus_df['headline_score'].apply(vader_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58deca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns difference between 0-2\n",
    "corpus2['sentiment_diff'] = abs(corpus2['vader_to_lstm'] - corpus2['content_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd590c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2['sentiment_mismatch'] = corpus2['vader_to_lstm'] != corpus2['content_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d28fe",
   "metadata": {},
   "source": [
    "## Visualising sentiment and source and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"same\", 1: \"more postive/negative\", 2: \"opposite\"}\n",
    "\n",
    "ax = sns.countplot(x='sentiment_diff', data=corpus2, palette='muted')\n",
    "plt.title('Difference Between Headline and Content Sentiment')\n",
    "ax.set_xticklabels([label_map.get(int(label.get_text()), \"\") for label in ax.get_xticklabels()])\n",
    "plt.xlabel('Sentiment Difference (Headline - Content)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c273d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "cols_to_fix = ['headline_people', 'headline_parties', 'content_people', 'content_parties']\n",
    "for col in cols_to_fix:\n",
    "    corpus2[col] = corpus2[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91049dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "leader_aliases = {\n",
    "    \"Albanese\": \"Anthony Albanese\",\n",
    "    \"Anthony Albanese\": \"Anthony Albanese\",\n",
    "    \"Dutton\": \"Peter Dutton\",\n",
    "    \"Peter Dutton\": \"Peter Dutton\",\n",
    "    \"Bandt\": \"Adam Bandt\",\n",
    "    \"Adam Bandt\": \"Adam Bandt\",\n",
    "    \"Morrison\": \"Scott Morrison\",\n",
    "    \"Scott Morrison\": \"Scott Morrison\",\n",
    "    \"Marles\": \"Richard Marles\",\n",
    "    \"Richard Marles\": \"Richard Marles\",\n",
    "    \"Wong\": \"Penny Wong\",\n",
    "    \"Penny Wong\": \"Penny Wong\"\n",
    "}\n",
    "\n",
    "def normalize_leaders(person_list):\n",
    "    return [leader_aliases[name] for name in person_list if name in leader_aliases]\n",
    "\n",
    "corpus2['headline_people'] = corpus2['headline_people'].apply(normalize_leaders)\n",
    "filtered_df = corpus2[corpus2['headline_people'].apply(lambda x: len(x) > 0)]\n",
    "filtered_df.to_csv('filtered_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f88dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt\n",
    "puor_colors = cm.get_cmap('PuOr', 3)\n",
    "colors = [puor_colors(i) for i in range(3)]  # RGBA tuples\n",
    "colors_hex = [plt.matplotlib.colors.to_hex(c) for c in colors]\n",
    "\n",
    "exploded_df = filtered_df.explode('headline_people')\n",
    "sentiment_counts = exploded_df.groupby(['headline_people', 'headline_sentiment']).size().unstack(fill_value=0)\n",
    "sentiment_counts.plot(kind = 'bar', colormap = 'PuOr', width = 1)\n",
    "plt.xlabel('NER political leaders')\n",
    "plt.ylabel('VADER Sentiment Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20503838",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counts = exploded_df.groupby(['source', 'headline_sentiment']).size().unstack(fill_value=0)\n",
    "source_counts.plot(kind = 'bar', colormap = 'PuOr', width = 1)\n",
    "plt.legend()\n",
    "plt.ylabel('VADER Sentiment Count')\n",
    "plt.xlabel('News Outlet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be823a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_and_leader_counts = exploded_df.groupby(['source', 'headline_sentiment', 'headline_people']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_and_leader_counts[['Anthony Albanese', 'Peter Dutton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_order = ['negative', 'neutral', 'positive']\n",
    "corpus_df['headline_sentiment'] = pd.Categorical(\n",
    "    corpus_df['headline_sentiment'],\n",
    "    categories=sentiment_order,\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97800cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    data=corpus_df,\n",
    "    x='headline_sentiment',\n",
    "    order=sentiment_order,\n",
    "    palette=colors\n",
    ")\n",
    "plt.xlabel('Headline Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    data=corpus_df,\n",
    "    x='source',\n",
    "    order=['theguardian', 'abc', 'skynews'],\n",
    "    palette=colors\n",
    ")\n",
    "plt.xlabel('News Outlet')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Corpus Articles by News Outlet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37987a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    data=corpus2.iloc[idx_train],\n",
    "    x='source',\n",
    "    order=['theguardian', 'abc', 'skynews'],\n",
    "    palette=colors\n",
    ")\n",
    "plt.xlabel('News Outlet')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sample Articles by News Outlet')\n",
    "plt.show()\n",
    "#sns.histplot(corpus2.iloc[idx_train]['source'], color='#b76b3e')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac176b4",
   "metadata": {},
   "source": [
    "# Bias Classifier\n",
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download NLTK resources\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"tokenizer and preprocessing using NLTK\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic words, then lemmatize\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower()) \n",
    "        for token in tokens \n",
    "        if token.isalpha() and token.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Vectorize the headlines using TF-IDF\n",
    "vectorizer = TfidfVectorizer(tokenizer=nltk_tokenizer, lowercase=False)\n",
    "X_tfidf = vectorizer.fit_transform(corpus_df['headline'])\n",
    "\n",
    "# Encode the target labels using onehot encoding\n",
    "y_encoded = pd.get_dummies(corpus_df['source']).to_numpy()  # Encode the categorical labels\n",
    "y_encoded_flat = np.argmax(y_encoded, axis=1)\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded_flat, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model_lr = LogisticRegression(max_iter=1000, multi_class='ovr')  # 'ovr' for multi-class\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model using classification metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea30ac6",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "headlines = corpus_df['headline'].tolist()\n",
    "\n",
    "max_len = max(len(bert_tokenizer.tokenize(h)) for h in headlines)\n",
    "\n",
    "\n",
    "def get_bert_embeddings(text_list, max_len=max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes and returns BERT embeddings for a list of texts with optional max_len.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize with dynamic max length padding\n",
    "    inputs = bert_tokenizer(\n",
    "        text_list,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt' # pytorch tensors\n",
    "    )\n",
    "\n",
    "    with torch.no_grad(): # does not compute gradients\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    # Shape: (batch_size, max_len, 768)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "def get_bert_embeddings_numpy(text_list, max_len=None):\n",
    "    \"\"\"\n",
    "    Same as get_bert_embeddings but returns NumPy array.\n",
    "    \"\"\"\n",
    "    embeddings = get_bert_embeddings(text_list, max_len)\n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "bert_headline_embeddings = get_bert_embeddings(headlines)\n",
    "bert_headline_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49fc64",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_model(units=128, learning_rate=0.001, drop_rate=0.2, dense_units=64):\n",
    "    \"\"\"LSTM model for tuning\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, input_shape=(32, 768)))  # change for BERT or ELMo input\n",
    "    model.add(Dropout(rate=drop_rate))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))  # 3 classes in output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def grid_search_model(X, y, param_grid, verbose=0):\n",
    "    \"\"\"Function for hyperparameter tuning\"\"\"\n",
    "    # Fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Create the KerasClassifier\n",
    "    model = KerasClassifier(model=create_model, epochs=10, batch_size=16, verbose=0)\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "    # Fit the grid search with early stop depending on loss\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    grid_result = grid.fit(X, y, callbacks=[early_stop], validation_split=0.2)\n",
    "\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    # Show the mean and standard deviation of each parameter setting\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    return grid_result.best_params_, grid_result.best_score_\n",
    "\n",
    "# Define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "unit_size = [64, 128]  # Different sizes for LSTM units\n",
    "drop_rate = [0.2, 0.3, 0.5]\n",
    "dense_units = [16, 32, 64]\n",
    "\n",
    "# Param grid for grid search\n",
    "param_grid = dict(optimizer__learning_rate=learn_rate, \n",
    "                  model__units=unit_size,\n",
    "                  model__drop_rate=drop_rate, \n",
    "                  model__dense_units=dense_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_bert, best_score_bert = grid_search_model(bert_headline_embeddings, y_source, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_elmo, best_score_elmo = grid_search_model(bert_headline_embeddings, y_source, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf805126",
   "metadata": {},
   "source": [
    "## Fitting Optimised Models\n",
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2079c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_source = pd.get_dummies(corpus_df['source']).to_numpy()\n",
    "def train_lstm_classifier(X, y, max_length, embedding_dim, num_classes=3, \n",
    "                          lstm_units=64, dropout_rate=0.3, dense_units=32,\n",
    "                          batch_size=16, epochs=6, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Train an LSTM model on the given data.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Input embeddings of shape (samples, max_length, embedding_dim)\n",
    "        y (ndarray): One-hot encoded labels\n",
    "        max_length (int): Max sequence length\n",
    "        embedding_dim (int): Embedding dimensionality (e.g. 1024 for ELMo, 768 for BERT)\n",
    "        num_classes (int): Number of output classes\n",
    "        lstm_units (int): Number of LSTM units\n",
    "        dropout_rate (float): Dropout rate\n",
    "        dense_units (int): Units in dense layer\n",
    "        batch_size (int): Batch size\n",
    "        epochs (int): Number of training epochs\n",
    "        validation_split (float): Fraction of data used for validation\n",
    "    \n",
    "    Returns:\n",
    "        model (tf.keras.Model): Trained model\n",
    "        history: Training history\n",
    "        evaluation: (loss, accuracy) on test set\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(X))\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        X, y, indices, test_size=validation_split, random_state=42\n",
    "    )\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(max_length, embedding_dim)),\n",
    "        tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test))\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    evaluation = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "    return model, history, evaluation, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert, history_bert, eval_bert, y_pred_bert, y_true_bert = train_lstm_classifier(\n",
    "    X=bert_headline_embeddings,\n",
    "    y=y_source,\n",
    "    max_length=max_len,  # dynamically determined\n",
    "    embedding_dim=768\n",
    ")\n",
    "print(\"BERT Report\")\n",
    "print(classification_report(y_true_bert, y_pred_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea966db",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best: 0.711185 using {'model__dense_units': 32, 'model__drop_rate': 0.2, 'model__units': 64, 'optimizer__learning_rate': 0.01}\n",
    "model_elmo, history_elmo, eval_elmo, y_pred_elmo, y_true_elmo = train_lstm_classifier(\n",
    "    X=elmo_headline_embeddings,\n",
    "    y=y_source,\n",
    "    max_length=29,\n",
    "    embedding_dim=1024\n",
    ")\n",
    "print(\"ELMo Report\")\n",
    "print(classification_report(y_true_elmo, y_pred_elmo))\n",
    "plot_training_history(history_elmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee13fc0",
   "metadata": {},
   "source": [
    "## Extra Optional XGBoost Model (ran out of words to include this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9421b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_cls, y_source, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train an XGBoost model on the embeddings\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
    "print(classification_report(y_test_xgb, y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
