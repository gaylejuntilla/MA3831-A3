{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb127f25",
   "metadata": {},
   "source": [
    "# Sky News Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58fa3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 9515212] copy template from The Guardian\r\n",
      " 1 file changed, 68 insertions(+), 2 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git add sky_news.ipynb\n",
    "!git commit -m \"copy \"\n",
    "#!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e674b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to check publish date during pagination\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class SkyNewsScraper:\n",
    "    def __init__(self, start_url, cutoff_date=\"1 January 2025\"):\n",
    "        self.start_url = start_url\n",
    "        self.cutoff_date = datetime.strptime(cutoff_date, \"%d %B %Y\")  # Convert string to datetime\n",
    "        self.articles = pd.DataFrame(columns=['headline', 'title', 'content', 'date', 'url'])  # DataFrame to store article data\n",
    "        self.current_page = 1\n",
    "        \n",
    "    def scrape_articles(self, url):\n",
    "        \"\"\"Scrapes articles from the page and handles pagination.\"\"\"\n",
    "        while url:\n",
    "        #while True:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            articles = soup.find_all('article')\n",
    "            if not articles:\n",
    "                print(\"No articles found on page, stopping...\")\n",
    "                break\n",
    "\n",
    "            last_article_date = None  # Track the last article's date\n",
    "            \n",
    "            for article in articles:\n",
    "                try:\n",
    "                    # Extract article info\n",
    "                    article_info = self.fetch_article_info(article)\n",
    "                    article_date = datetime.strptime(article_info['date'], \"%d %B %Y\")  # Convert to datetime object\n",
    "                    \n",
    "                    # Check if this article is older than cutoff_date\n",
    "                    if article_date < self.cutoff_date:\n",
    "                        print(f\"Reached article older than cutoff date: {article_info['date']}, stopping...\")\n",
    "                        return  # Exit function to stop scraping\n",
    "\n",
    "                    # Scrape article details\n",
    "                    title, content = self.scrape_article_details(article_info['article_url'])\n",
    "\n",
    "                    if title and content:\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'headline': [article_info['headline']],\n",
    "                            'title': [title],\n",
    "                            'content': [content],\n",
    "                            'date': [article_info['date']],\n",
    "                            'url': [article_info['article_url']]\n",
    "                        })\n",
    "                        self.articles = pd.concat([self.articles, new_row], ignore_index=True)\n",
    "                        print(f\"Collected: {title}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping article: {title}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "\n",
    "                time.sleep(5)\n",
    "                \n",
    "            #break\n",
    "            # Step 2: Find the next page (pagination)\n",
    "            url = self.get_next_page(soup)\n",
    "            if not url:\n",
    "                print(\"No more pages found, stopping...\")\n",
    "                break\n",
    "            self.current_page += 1\n",
    "    \n",
    "    def fetch_article_info(self, article):\n",
    "        \"\"\"Helper method to extract article information like headline and URL.\"\"\"\n",
    "        attributes = article.find('h4', {'class': \"storyblock_title\"})\n",
    "        article_url = attributes.find('a')['href']\n",
    "        headline = attributes.find('a').text\n",
    "        date_str = article.find('time')['datetime']  # Extract the date string\n",
    "        date_obj = datetime.strptime(date_str, \"%d/%m/%Y\")  # Convert to datetime object\n",
    "        date = date_obj.strftime(\"%d %B %Y\")\n",
    "\n",
    "        return {'headline': headline, 'article_url': article_url, 'date': date} \n",
    "\n",
    "    def scrape_article_details(self, article_url):\n",
    "        \"\"\"Scrapes article content, title, date, and other details from individual article pages.\"\"\"\n",
    "        response = requests.get(article_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            # Extracting title\n",
    "            title = soup.find('h1').text\n",
    "\n",
    "            # Extracting content (Assuming paragraphs are in 'div' with class 'content__article-body')\n",
    "            content_element = soup.find('div', {'id': 'story-primary'})\n",
    "            paragraphs = content_element.find_all('p')\n",
    "            content = \" \".join([p.text for p in paragraphs]).strip()\n",
    "\n",
    "            return title, content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "            return None, None, None  # Return None in case of an error\n",
    "\n",
    "    def get_next_page(self, soup):\n",
    "        \"\"\"Finds the next page URL from pagination links at the bottom of the page.\"\"\"\n",
    "        pagination_links = soup.find('ul', {'class': 'page-numbers'})  # Find pagination section\n",
    "\n",
    "        if pagination_links:\n",
    "            all_pages = pagination_links.find_all('a', text=re.compile(r'\\d+'))  # Find all numbered page links\n",
    "\n",
    "            # Otherwise, continue to the next page in sequence\n",
    "            for page_link in all_pages:\n",
    "                if int(page_link.text.strip()) == self.current_page + 1:\n",
    "                    return page_link['href']  # Get the next sequential page\n",
    "\n",
    "    def save_data_to_file(self, filename='guardian_articles.csv'):\n",
    "        \"\"\"Saves the collected articles to a CSV file.\"\"\"\n",
    "        self.articles.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
